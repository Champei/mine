{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Champei/mine/blob/main/task2/jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. INSTALL\n",
        "\n",
        "!pip install torch torchaudio torchvision transformers tqdm matplotlib\n"
      ],
      "metadata": {
        "id": "M8t5ZYzPWGpL"
      },
      "id": "M8t5ZYzPWGpL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. MOUNT GOOGLE DRIVE & VERIFY DATA\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Audios'\n",
        "\n",
        "print(\"Subfolders (categories):\", os.listdir(BASE_PATH))\n"
      ],
      "metadata": {
        "id": "_9db38oiW1eT"
      },
      "id": "_9db38oiW1eT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. IMPORTS & DATASET CLASS\n",
        "\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio, random, matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "class TrainAudioSpectrogramDataset(Dataset):\n",
        "    \"\"\"Loads .wav files, converts to log-mel spectrograms, returns tensor + one-hot label.\"\"\"\n",
        "    def __init__(self, root_dir, categories, max_frames=512, fraction=1.0):\n",
        "        self.root_dir, self.categories, self.max_frames = root_dir, categories, max_frames\n",
        "        self.file_list = []\n",
        "        self.class_to_idx = {cat: i for i, cat in enumerate(categories)}\n",
        "\n",
        "        for cat in categories:\n",
        "            path = os.path.join(root_dir, cat)\n",
        "            files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.wav')]\n",
        "            n = int(len(files) * fraction)\n",
        "            for f in random.sample(files, n):\n",
        "                self.file_list.append((f, self.class_to_idx[cat]))\n",
        "\n",
        "    def __len__(self): return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.file_list[idx]\n",
        "        wav, sr = torchaudio.load(path)\n",
        "        if wav.size(0) > 1: wav = wav.mean(0, keepdim=True)\n",
        "        mel = torchaudio.transforms.MelSpectrogram(sr, n_fft=1024, hop_length=256, n_mels=128)(wav)\n",
        "        logmel = torch.log1p(mel)\n",
        "        _, _, n_frames = logmel.shape\n",
        "        logmel = F.pad(logmel, (0, max(0, 512 - n_frames)))[:, :, :512]\n",
        "        y = F.one_hot(torch.tensor(label), num_classes=len(self.categories)).float()\n",
        "        return logmel, y\n"
      ],
      "metadata": {
        "id": "tomGOun8YIzZ"
      },
      "id": "tomGOun8YIzZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. MODELS\n",
        "\n",
        "class CGAN_Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.latent_dim, self.num_classes = latent_dim, num_classes\n",
        "        self.num_categories = num_classes\n",
        "        self.fc = nn.Linear(latent_dim + num_classes, 256 * 8 * 32)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64,32,4,2,1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32,1,4,2,1), nn.ReLU()\n",
        "        )\n",
        "    def forward(self,z,y):\n",
        "        h=torch.cat([z,y],1)\n",
        "        h=self.fc(h).view(-1,256,8,32)\n",
        "        return self.net(h)\n",
        "\n",
        "class CGAN_Discriminator(nn.Module):\n",
        "    def __init__(self,num_classes):\n",
        "        super().__init__()\n",
        "        self.label_emb=nn.Linear(num_classes,128*512)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Conv2d(2,32,4,2,1), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32,64,4,2,1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64,128,4,2,1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128,256,4,2,1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256,1,(8,32),1,0)\n",
        "        )\n",
        "    def forward(self,x,y):\n",
        "        ymap=self.label_emb(y).view(-1,1,128,512)\n",
        "        h=torch.cat([x,ymap],1)\n",
        "        return self.net(h).view(-1,1)\n"
      ],
      "metadata": {
        "id": "fcdl1JOcaEl5"
      },
      "id": "fcdl1JOcaEl5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. UTILITIES FOR AUDIO GENERATION & PLAYBACK\n",
        "\n",
        "def generate_audio_gan(generator, category_idx, num_samples, device, sr=22050):\n",
        "    generator.eval()\n",
        "    y = F.one_hot(torch.tensor([category_idx]), num_classes=generator.num_classes).float().to(device)\n",
        "    z = torch.randn(num_samples, generator.latent_dim, device=device)\n",
        "    with torch.no_grad():\n",
        "        logmel = generator(z, y)\n",
        "    mel = torch.expm1(logmel).squeeze(1)\n",
        "    invmel = torchaudio.transforms.InverseMelScale(n_stft=513, n_mels=128, sample_rate=sr).to(device)\n",
        "    spec = invmel(mel)\n",
        "    griffin = torchaudio.transforms.GriffinLim(1024, hop_length=256, n_iter=32).to(device)\n",
        "    wav = griffin(spec).cpu()\n",
        "    return wav\n",
        "\n",
        "def play_and_save(wav, sr, name):\n",
        "    import torchaudio\n",
        "    torchaudio.save(name, wav.squeeze(0), sr)\n",
        "    print(\"Saved:\", name)\n",
        "    display(Audio(wav.numpy().squeeze(), rate=sr))\n"
      ],
      "metadata": {
        "id": "358iVK8oaSF8"
      },
      "id": "358iVK8oaSF8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. TRAINING FUNCTION\n",
        "\n",
        "def train_gan(generator, discriminator, dataloader, device, categories, epochs, lr, latent_dim):\n",
        "\n",
        "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    os.makedirs(\"gan_spectrogram_plots\", exist_ok=True)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
        "\n",
        "        for real_specs, labels in loop:\n",
        "            real_specs = real_specs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = real_specs.size(0)\n",
        "\n",
        "            real_labels = torch.ones(batch_size, 1, device=device)\n",
        "            fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "            # Train Discriminator\n",
        "\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            real_out = discriminator(real_specs, labels)\n",
        "            loss_D_real = criterion(real_out, real_labels)\n",
        "\n",
        "            z = torch.randn(batch_size, latent_dim, device=device)\n",
        "            fake_specs = generator(z, labels)\n",
        "\n",
        "            fake_out = discriminator(fake_specs.detach(), labels)\n",
        "            loss_D_fake = criterion(fake_out, fake_labels)\n",
        "\n",
        "            loss_D = loss_D_real + loss_D_fake\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Train Generator\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            fake_out = discriminator(fake_specs, labels)\n",
        "            loss_G = criterion(fake_out, real_labels)\n",
        "\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            loop.set_postfix(lossD=loss_D.item(), lossG=loss_G.item())\n",
        "\n",
        "        print(f\"\\nSample generation after epoch {epoch}\")\n",
        "\n",
        "        generator.eval()\n",
        "\n",
        "        for cat_idx, cat_name in enumerate(categories):\n",
        "            y_cond = F.one_hot(torch.tensor([cat_idx]), num_classes=generator.num_categories).float().to(device)\n",
        "            z_sample = torch.randn(1, generator.latent_dim).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                spec_log = generator(z_sample, y_cond).squeeze().cpu().numpy()\n",
        "\n",
        "            plt.figure(figsize=(6,4))\n",
        "            plt.imshow(spec_log, aspect='auto', origin='lower', cmap='viridis')\n",
        "            plt.title(f\"{cat_name} (Epoch {epoch})\")\n",
        "            plt.axis('off')\n",
        "            plt.savefig(f\"gan_spectrogram_plots/{cat_name}_ep{epoch}.png\")\n",
        "            plt.close()\n",
        "\n",
        "        generator.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "znISdU_SaZAk"
      },
      "id": "znISdU_SaZAk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install ffmpeg -y\n"
      ],
      "metadata": {
        "id": "5jugQX5XbKEF"
      },
      "id": "5jugQX5XbKEF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Audios'\n",
        "\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for f in files:\n",
        "        if f.lower().endswith('.mp3'):\n",
        "            mp3_path = os.path.join(root, f)\n",
        "            wav_path = os.path.splitext(mp3_path)[0] + '.wav'\n",
        "\n",
        "            # Only convert if WAV doesn't exist yet\n",
        "            if not os.path.exists(wav_path):\n",
        "                print(f\"Converting: {mp3_path}\")\n",
        "                !ffmpeg -y -i \"{mp3_path}\" -ar 22050 -ac 1 \"{wav_path}\"\n"
      ],
      "metadata": {
        "id": "_vk1cX6DbZ4z"
      },
      "id": "_vk1cX6DbZ4z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in os.listdir(base_dir):\n",
        "    cat_path = os.path.join(base_dir, cat)\n",
        "    if os.path.isdir(cat_path):\n",
        "        wavs = [f for f in os.listdir(cat_path) if f.endswith('.wav')]\n",
        "        print(f\"{cat}: {len(wavs)} wav files\")\n"
      ],
      "metadata": {
        "id": "GwKaLtGxbcf5"
      },
      "id": "GwKaLtGxbcf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LATENT_DIM = 100\n",
        "EPOCHS = 5\n",
        "BATCH = 8\n",
        "LR = 2e-4"
      ],
      "metadata": {
        "id": "Tz5-19d6biPK"
      },
      "id": "Tz5-19d6biPK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/MyDrive/Audios'\n",
        "cats = sorted([d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))])\n",
        "print(\"Categories:\", cats)\n",
        "\n",
        "ds = TrainAudioSpectrogramDataset(train_path, cats)\n",
        "dl = DataLoader(ds, batch_size=BATCH, shuffle=True, num_workers=2)\n",
        "\n",
        "G = CGAN_Generator(LATENT_DIM, len(cats)).to(DEVICE)\n",
        "D = CGAN_Discriminator(len(cats)).to(DEVICE)\n",
        "\n",
        "train_gan(G, D, dl, DEVICE, cats, EPOCHS, LR, LATENT_DIM)\n"
      ],
      "metadata": {
        "id": "JL_lNQw2bf1M"
      },
      "id": "JL_lNQw2bf1M",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}